{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNJVCOMvvlBqN4YZ9c0f3MH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":[" !pip install config"],"metadata":{"id":"QkPBwxX8G7s8","executionInfo":{"status":"ok","timestamp":1682423842862,"user_tz":-180,"elapsed":6887,"user":{"displayName":"Berçin Saba Güngör (Student)","userId":"02452825459021664904"}},"outputId":"6bd12174-1986-4b62-f37e-a76c9453cc48","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting config\n","  Downloading config-0.5.1-py2.py3-none-any.whl (20 kB)\n","Installing collected packages: config\n","Successfully installed config-0.5.1\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"ZAOckGA7GbDd","executionInfo":{"status":"error","timestamp":1682423873735,"user_tz":-180,"elapsed":1133,"user":{"displayName":"Berçin Saba Güngör (Student)","userId":"02452825459021664904"}},"outputId":"b71d71ed-c88d-4c06-dd34-91a1cf93c71a"},"outputs":[{"output_type":"error","ename":"ImportError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-46b9631445f5>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muuid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLINK_LIST_PATH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Encoding for writing the URLs to the .txt file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'LINK_LIST_PATH' from 'config' (/usr/local/lib/python3.9/dist-packages/config/__init__.py)","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["import os, sys, glob, re\n","import json\n","from pprint import pprint\n","\n","import requests\n","from bs4 import BeautifulSoup as bs\n","import pandas as pd\n","import numpy as np\n","import uuid\n","\n","from config import LINK_LIST_PATH\n","\n","# Encoding for writing the URLs to the .txt file\n","# Do not change unless you are getting a UnicodeEncodeError\n","ENCODING = \"utf-8\"\n","\n","\n","def save_link(url, page):\n","    \"\"\"\n","    Save collected link/url and page to the .txt file in LINK_LIST_PATH\n","    \"\"\"\n","    id_str = uuid.uuid3(uuid.NAMESPACE_URL, url).hex\n","    with open(LINK_LIST_PATH, \"a\", encoding=ENCODING) as f:\n","        f.write(\"\\t\".join([id_str, url, str(page)]) + \"\\n\")\n","\n","\n","def download_links_from_index():\n","    \"\"\"\n","    This function should go to the defined \"url\" and download the news page links from all\n","    pages and save them into a .txt file.\n","    \"\"\"\n","\n","    # Checking if the link_list.txt file exists\n","    if not os.path.exists(LINK_LIST_PATH):\n","        with open(LINK_LIST_PATH, \"w\", encoding=ENCODING) as f:\n","            f.write(\"\\t\".join([\"id\", \"url\", \"page\"]) + \"\\n\")\n","        start_page = 1\n","        downloaded_url_list = []\n","\n","    # If some links have already been downloaded,\n","    # get the downloaded links and start page\n","    else:\n","        # Get the page to start from\n","        data = pd.read_csv(LINK_LIST_PATH, sep=\"\\t\")\n","        if data.shape[0] == 0:\n","            start_page = 1\n","            downloaded_url_list = []\n","        else:\n","            start_page = data[\"page\"].astype(\"int\").max()\n","            downloaded_url_list = data[\"url\"].to_list()\n","\n","    # WRITE YOUR CODE HERE\n","    #########################################\n","    # Start downloading from the page \"start_page\"\n","    # which is the page you ended at the last\n","    # time you ran the code (if you had an error and the code stopped)\n","\n","    # Save the collected url in the variable \"collected_url\"\n","    collected_url = \"\"\n","\n","    # Save the page that the url is taken from in the variable \"page\"\n","    page = \"\"\n","\n","    # The following code block saves the collected url and page\n","    # Save the collected urls one by one so that if an error occurs\n","    # you do not have to start all over again\n","    if collected_url not in downloaded_url_list:\n","        print(\"\\t\", collected_url, flush=True)\n","        save_link(collected_url, page)\n","    #########################################\n","\n","\n","if __name__ == \"__main__\":\n","    download_links_from_index()\n"]}]}